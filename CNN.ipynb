{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the 'NN' notebook, but will use convolutional neural networks instead of normal neural nets on flattened arrays of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# The data that we are interested in is made of 8x8 images of digits, let's\n",
    "# have a look at the first 4 images, stored in the `images` attribute of the\n",
    "# dataset.  If we were working from image files, we could load them using\n",
    "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
    "# images, we know which digit they represent: it is given in the 'target' of\n",
    "# the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAB4CAYAAADSWhi9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkvkfiDN/okZBD1cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123930860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_and_labels = list(zip(digits.images, digits.target))\n",
    "for index, (image, label) in enumerate(images_and_labels[:4]):\n",
    "    plt.subplot(2, 4, index + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title('Training: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To apply a classifier on this data, we need to flatten the image, to\n",
    "# turn the data in a (samples, feature) matrix:\n",
    "from keras.utils.np_utils import to_categorical\n",
    "digits.target_cat = to_categorical(digits.target)\n",
    "\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images\n",
    "X_train = data[:n_samples // 2]\n",
    "X_test = data[n_samples // 2:]\n",
    "y_train = digits.target_cat[:n_samples // 2]\n",
    "y_test = digits.target_cat[n_samples // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEQCAYAAADlK+DYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naFAAAJf+D5tP0qiuavZ+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122c06588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=5)\n",
    "for j in range(3):\n",
    "    for i in range(5):\n",
    "        axes[j, i].imshow(X_train[j*5 + i])\n",
    "        axes[j, i].set_xticklabels([])\n",
    "        axes[j, i].set_yticklabels([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.reshape(X_train.shape[0], 1, X_train.shape[1], X_train.shape[2]) #reshape to have depth of 1\n",
    "X_test2 = X_test.reshape(X_test.shape[0], 1, X_test.shape[1], X_test.shape[2]) #need that for convolutions\n",
    "X_train2 = X_train2.astype('float32') #make it float\n",
    "X_test2 = X_test2.astype('float32')\n",
    "X_train2 /= 16 #scale 0 to 1\n",
    "X_test2 /= 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using https://elitedatascience.com/keras-tutorial-deep-learning-in-python as a guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/muneebalam/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (2, 2), activation=\"relu\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 628 samples, validate on 270 samples\n",
      "Epoch 1/50\n",
      "628/628 [==============================] - 0s - loss: 2.2557 - acc: 0.1545 - val_loss: 2.2051 - val_acc: 0.1889\n",
      "Epoch 2/50\n",
      "628/628 [==============================] - 0s - loss: 2.1600 - acc: 0.2834 - val_loss: 2.1003 - val_acc: 0.4370\n",
      "Epoch 3/50\n",
      "628/628 [==============================] - 0s - loss: 2.0308 - acc: 0.4602 - val_loss: 1.9538 - val_acc: 0.5407\n",
      "Epoch 4/50\n",
      "628/628 [==============================] - 0s - loss: 1.8479 - acc: 0.5717 - val_loss: 1.7591 - val_acc: 0.6704\n",
      "Epoch 5/50\n",
      "628/628 [==============================] - 0s - loss: 1.6320 - acc: 0.6640 - val_loss: 1.5233 - val_acc: 0.7481\n",
      "Epoch 6/50\n",
      "628/628 [==============================] - 0s - loss: 1.3949 - acc: 0.7102 - val_loss: 1.2816 - val_acc: 0.8037\n",
      "Epoch 7/50\n",
      "628/628 [==============================] - 0s - loss: 1.1662 - acc: 0.7675 - val_loss: 1.0747 - val_acc: 0.8000\n",
      "Epoch 8/50\n",
      "628/628 [==============================] - 0s - loss: 1.0082 - acc: 0.7755 - val_loss: 0.9147 - val_acc: 0.8259\n",
      "Epoch 9/50\n",
      "628/628 [==============================] - 0s - loss: 0.8806 - acc: 0.8010 - val_loss: 0.7972 - val_acc: 0.8333\n",
      "Epoch 10/50\n",
      "628/628 [==============================] - 0s - loss: 0.7389 - acc: 0.8376 - val_loss: 0.7163 - val_acc: 0.8407\n",
      "Epoch 11/50\n",
      "628/628 [==============================] - 0s - loss: 0.6658 - acc: 0.8344 - val_loss: 0.6542 - val_acc: 0.8444\n",
      "Epoch 12/50\n",
      "628/628 [==============================] - 0s - loss: 0.5950 - acc: 0.8455 - val_loss: 0.6031 - val_acc: 0.8481\n",
      "Epoch 13/50\n",
      "628/628 [==============================] - 0s - loss: 0.5280 - acc: 0.8615 - val_loss: 0.5743 - val_acc: 0.8519\n",
      "Epoch 14/50\n",
      "628/628 [==============================] - 0s - loss: 0.5117 - acc: 0.8392 - val_loss: 0.5427 - val_acc: 0.8556\n",
      "Epoch 15/50\n",
      "628/628 [==============================] - 0s - loss: 0.4478 - acc: 0.8949 - val_loss: 0.5222 - val_acc: 0.8593\n",
      "Epoch 16/50\n",
      "628/628 [==============================] - 0s - loss: 0.4293 - acc: 0.8662 - val_loss: 0.4944 - val_acc: 0.8593\n",
      "Epoch 17/50\n",
      "628/628 [==============================] - 0s - loss: 0.3973 - acc: 0.8901 - val_loss: 0.4824 - val_acc: 0.8667\n",
      "Epoch 18/50\n",
      "628/628 [==============================] - 0s - loss: 0.4062 - acc: 0.8822 - val_loss: 0.4539 - val_acc: 0.8667\n",
      "Epoch 19/50\n",
      "628/628 [==============================] - 0s - loss: 0.3394 - acc: 0.9172 - val_loss: 0.4509 - val_acc: 0.8704\n",
      "Epoch 20/50\n",
      "628/628 [==============================] - 0s - loss: 0.3337 - acc: 0.9029 - val_loss: 0.4349 - val_acc: 0.8704\n",
      "Epoch 21/50\n",
      "628/628 [==============================] - 0s - loss: 0.2926 - acc: 0.9283 - val_loss: 0.4184 - val_acc: 0.8704\n",
      "Epoch 22/50\n",
      "628/628 [==============================] - 0s - loss: 0.3099 - acc: 0.9076 - val_loss: 0.4133 - val_acc: 0.8667\n",
      "Epoch 23/50\n",
      "628/628 [==============================] - 0s - loss: 0.2759 - acc: 0.9220 - val_loss: 0.4002 - val_acc: 0.8704\n",
      "Epoch 24/50\n",
      "628/628 [==============================] - 0s - loss: 0.2550 - acc: 0.9283 - val_loss: 0.4005 - val_acc: 0.8667\n",
      "Epoch 25/50\n",
      "628/628 [==============================] - 0s - loss: 0.2363 - acc: 0.9443 - val_loss: 0.3954 - val_acc: 0.8667\n",
      "Epoch 26/50\n",
      "628/628 [==============================] - 0s - loss: 0.2317 - acc: 0.9443 - val_loss: 0.3767 - val_acc: 0.8741\n",
      "Epoch 27/50\n",
      "628/628 [==============================] - 0s - loss: 0.2274 - acc: 0.9427 - val_loss: 0.3716 - val_acc: 0.8667\n",
      "Epoch 28/50\n",
      "628/628 [==============================] - 0s - loss: 0.2193 - acc: 0.9490 - val_loss: 0.3684 - val_acc: 0.8741\n",
      "Epoch 29/50\n",
      "628/628 [==============================] - 0s - loss: 0.2255 - acc: 0.9427 - val_loss: 0.3647 - val_acc: 0.8704\n",
      "Epoch 30/50\n",
      "628/628 [==============================] - 0s - loss: 0.1894 - acc: 0.9522 - val_loss: 0.3468 - val_acc: 0.8815\n",
      "Epoch 31/50\n",
      "628/628 [==============================] - 0s - loss: 0.1670 - acc: 0.9634 - val_loss: 0.3464 - val_acc: 0.8741\n",
      "Epoch 32/50\n",
      "628/628 [==============================] - 0s - loss: 0.2051 - acc: 0.9427 - val_loss: 0.3325 - val_acc: 0.8815\n",
      "Epoch 33/50\n",
      "628/628 [==============================] - 0s - loss: 0.1633 - acc: 0.9554 - val_loss: 0.3499 - val_acc: 0.8704\n",
      "Epoch 34/50\n",
      "628/628 [==============================] - 0s - loss: 0.1665 - acc: 0.9490 - val_loss: 0.3358 - val_acc: 0.8704\n",
      "Epoch 35/50\n",
      "628/628 [==============================] - 0s - loss: 0.1622 - acc: 0.9506 - val_loss: 0.3074 - val_acc: 0.8889\n",
      "Epoch 36/50\n",
      "628/628 [==============================] - 0s - loss: 0.1658 - acc: 0.9602 - val_loss: 0.3163 - val_acc: 0.8852\n",
      "Epoch 37/50\n",
      "628/628 [==============================] - 0s - loss: 0.1609 - acc: 0.9570 - val_loss: 0.3046 - val_acc: 0.8926\n",
      "Epoch 38/50\n",
      "628/628 [==============================] - 0s - loss: 0.1366 - acc: 0.9713 - val_loss: 0.3152 - val_acc: 0.8889\n",
      "Epoch 39/50\n",
      "628/628 [==============================] - 0s - loss: 0.1457 - acc: 0.9634 - val_loss: 0.3077 - val_acc: 0.9000\n",
      "Epoch 40/50\n",
      "628/628 [==============================] - 0s - loss: 0.1382 - acc: 0.9634 - val_loss: 0.2992 - val_acc: 0.8963\n",
      "Epoch 41/50\n",
      "628/628 [==============================] - 0s - loss: 0.1313 - acc: 0.9650 - val_loss: 0.3018 - val_acc: 0.8889\n",
      "Epoch 42/50\n",
      "628/628 [==============================] - 0s - loss: 0.1325 - acc: 0.9682 - val_loss: 0.3181 - val_acc: 0.8667\n",
      "Epoch 43/50\n",
      "628/628 [==============================] - 0s - loss: 0.1365 - acc: 0.9586 - val_loss: 0.3004 - val_acc: 0.8889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122154eb8>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed\n",
    "seed(8)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, (2, 2), activation='relu', input_shape=X_train2.shape[1:], data_format='channels_first'))\n",
    "model.add(Conv2D(16, 2, 2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train2, y_train, validation_split=0.3, epochs=50, callbacks=[EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.98</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.80</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.91</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.88</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.95</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.87</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      precision recall f1-score support\n",
       "Digit                                  \n",
       "0          0.99   0.97     0.98      88\n",
       "1          0.90   0.73     0.80      91\n",
       "2          0.92   0.90     0.91      86\n",
       "3          0.90   0.86     0.88      91\n",
       "4          0.96   0.95     0.95      92\n",
       "5          0.93   0.97     0.95      91\n",
       "6          0.93   0.98     0.95      91\n",
       "7          0.94   0.85     0.89      89\n",
       "8          0.74   0.81     0.77      88\n",
       "9          0.80   0.96     0.87      92\n",
       "avg        None   None     None    None\n",
       "total      0.90   0.90     0.90     899"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test2)\n",
    "# The classification report required 1D arrays, so I'll convert everything to max probability\n",
    "from numpy import argmax\n",
    "predictions1D = argmax(predictions, axis=1) #should have shape (899,)\n",
    "y_test1D = argmax(y_test, axis=1)\n",
    "report = metrics.classification_report(y_test1D, predictions1D)\n",
    "\n",
    "import pandas as pd #for formatting\n",
    "df = pd.DataFrame([x.split() for x in report.replace('/', '\\n').split('\\n')]) #have a problematic slash in there\n",
    "from numpy import concatenate, array\n",
    "df.columns = concatenate([array(['Digit']), df.iloc[0,:-1]])\n",
    "df.drop(df.index[:2], inplace=True)\n",
    "df.set_index('Digit', inplace=True)\n",
    "df.drop(df.index[pd.isnull(df.index.values)], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like it stabilized around 90% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89543937708565069"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions1D == y_test1D)/len(y_test1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little better than the other neural network but still much worse than, say, SVM, which got to 97%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
